# -*- coding: utf-8 -*-
"""linear re using pyTorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j6NejiJ4sCXRjgyLXwyTpneddYxcT9Kj
"""

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#scalar
scalar= torch.tensor(7)
scalar

scalar.item()

#vector
vector=torch.tensor([7 , 7])
vector

#matrix
matrix=torch.tensor([[7 , 7] , [6, 6]])
matrix

matrix.ndim

matrix.shape

#tensor
TENSOR=torch.tensor([[[3, 4, 5], [5, 6, 7], [1, 3, 5]]])
TENSOR

TENSOR.ndim

TENSOR.shape

"""#Random tensors"""

random_tensor=torch.rand(3, 4)
random_tensor

random_tensor.ndim

random_tensor=torch.rand(2, 3, 4)
random_tensor

#create random tensor with similar shape to an image tensor
random_img_size_tensor = torch.rand(size = (224, 224, 3)) # height, width, colour channel(red , blue , green)
random_img_size_tensor

# create  a tensor all zeros and ones ke liye bhi same code
zeros = torch.zeros(size = (3 , 4))
zeros

"""#CREATING A RANGE OF TENSORS AND TENSOR-LIKE"""

# use torch.range()
torch.arange(1 , 11)

torch.arange(start=0 , end=1000 , step=55)

"""# Tensor datatype

there are many type of datatypes of tensor
    flot 32 tensor or flot_32_tensor
    flot 16 tensor or flot_16_tensor
    So we will convert flot_32_tensor to flot_16_tensor
    dtype=None
    device=None
     requires_grad=None)
"""

flot_32_tensor = torch.tensor([6.0 , 7.0 , 9.0])
flot_32_tensor

flot_16_tensor = flot_32_tensor.type(torch.float16)
flot_16_tensor

"""#Tensor attributes


1.   object.dtype
2.   object.device
3.   object.shape




"""

some_tensor = torch.rand(3 , 4)
some_tensor

print(some_tensor)
print(f"datatype of tensor : {some_tensor.dtype}")
print(f"shape of tensor : {some_tensor.shape}")
print(f" device of tensor : {some_tensor.device}")

"""# tensor operation
1. add , sub , div , multi
"""

tensor = torch.tensor([3 , 4])
tensor
tensor + 10

tensor = torch.tensor([3 , 4])
tensor*10

torch.mul(tensor,10)

"""#Matrix multiplication"""

tensor= torch.tensor([2 , 4, 5 ])
tensor

# Commented out IPython magic to ensure Python compatibility.
# %%time
# total = 0
# for i in range(len(tensor)):
#   total+= tensor[i] * tensor[i]
# print(total)
# print(len(tensor))
# print(tensor[0])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# torch.matmul(tensor , tensor)

matrix_1 = torch.rand(3,10)
matrix_2 = torch.rand(10,3)

matrix_1

matrix_2

new_matrix = torch.matmul(matrix_1 , matrix_2)
new_matrix

new_matrix.shape
new_matrix.ndim

"""#data(preparing and loading"""

# create known parameters y=a(bias)+xb(weight)
weight=0.7
bias=0.3

start=0
step = 0.02
end=1
X= torch.arange(start, end , step).unsqueeze(dim=1)
y = weight*X + bias
X[:10] , y[:10] , len(X) , len(y)

"""#linear regression

#splitting data into training and test set
"""

train_split = int(0.8 * len(X))
train_split
X_train , y_train = X[:train_split] , y[:train_split]
X_test , y_test = X[train_split:] , y[train_split:]
len(X_train) , len(y_train) , len(X_test) , len(y_test)

def plot_prediction(train_data = X_train , train_labels = y_train ,
                    test_data = X_test , test_labels = y_test ,
                    prediction = None ):
  plt.scatter(train_data , train_labels , c="b" , s=4, label = "training data")
  plt.scatter(test_data , test_labels , c= "g", s=4 , label = "testing data")
  if prediction is not None :
    plt.scatter(test_data , prediction, c="r" , s=4 , label = "prediction")
plt.legend(prop={"size" : 14 });

plot_prediction()

from torch import nn # nn.module have all bulding block of nn and it is sub class
class linearRegressionModel(nn.Module):
  def __init__(self):
    super().__init__()
    self.weight=nn.Parameter(torch.randn(1,
                                         requires_grad=True ,
                                         dtype=torch.float
                                         ))
    self.bias= nn.Parameter(torch.randn(1,
                                         requires_grad=True ,
                                         dtype=torch.float
                                         ))
  def forward(self , X: torch.tensor):
    return self.weight *X +self.bias

"""#WHAT OUR MODEL DOES:
1. start with random values (weight and bias)
2. adjust the random values to better represent or get closer to ideal value

"""

torch.manual_seed(42)
model_0=linearRegressionModel()
list(model_0.parameters())

model_0.state_dict()

with torch.inference_mode():
  y_pred=model_0(X_test)

y_pred

y_test

plot_prediction(prediction=y_pred)

loss_fn= nn.L1Loss()
optimizer= torch.optim.SGD(params=model_0.parameters(),
                           lr=0.015)

model_0.eval()
with torch.inference_mode():
  list(model_0.parameters())

epochs=100
for epoch in range(epochs):
  model_0.train()
  #forward pass
  y_pred=model_0(X_train)
  # calculate loss
  loss= loss_fn(y_pred , y_train)
  print(f"loss : {loss}")
  #optimizer zero grad
  optimizer.zero_grad()
  #perform backpropogation
  loss.backward()
  #set optimizer
  optimizer.step()
  #testing
  model_0.eval()
  with torch.inference_mode():
    test_pred = model_0(X_test)

model_0.state_dict()

with torch.inference_mode():
  y_pred = model_0(X_test)

y_pred

plot_prediction(prediction=y_pred)

